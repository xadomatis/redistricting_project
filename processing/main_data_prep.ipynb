{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the district files into a single dataframe\n",
    "dist_path = \"/Users/xavier/Desktop/DSPP/solo_projects/redistricting_project/clean_data/full_districts\"\n",
    "all_files = sorted(glob.glob(dist_path + \"/*.csv\"))\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "districts = pd.concat(li, axis=0, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_path = \"/Users/xavier/Desktop/DSPP/solo_projects/redistricting_project/clean_data/state_pvi\"\n",
    "all_files = sorted(glob.glob(state_path + \"/*.csv\"))\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "all_states = pd.concat(li, axis=0, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = [2010,2014]\n",
    "blu = [2006,2008,2018]\n",
    "districts[\"wave\"] = \"Neutral\"\n",
    "districts[\"wave\"] = np.where(districts[\"year\"].isin(red), 'Red', districts[\"wave\"])\n",
    "districts[\"wave\"] = np.where(districts[\"year\"].isin(blu), 'Blue', districts[\"wave\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin the data and add the associated probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running multiple models on the data, it seemed that the raw probibilities (with some binning) were the best associated values. For all PVIs betwee D+5 and R+4 the probabilities speak for themselves and then for the more extreme ones are binned to remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataframe into the smoothed bins\n",
    "districts[\"bin\"] = pd.cut(districts['metric'], \n",
    "    [0, .24, .42, .44, .45, .46, .47, .48, .49, .5,\n",
    "        .51, .52, .53, .54, .57, .7, 1],\n",
    "    labels=[\"D+26+\",\"D+25 to D+8\",\"D+7 to D+6\",\"D+5\",\"D+4\",\"D+3\",\"D+2\",\"D+1\",\n",
    "        \"EVEN\",\"R+1\",\"R+2\",\"R+3\",\"R+4\",\"R+5 to R+7\",\"R+8 to R+20\",\"R+21+\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_probs(df):\n",
    "    '''Aggregate probailites of GOP represenation by bin\n",
    "    Args:\n",
    "        a datdistricts with historical election results\n",
    "    Returns:\n",
    "        a dataframe aggregating the probabilities of GOP representation\n",
    "    '''\n",
    "    sort = df.groupby(\"bin\").mean()\n",
    "    sort = sort.sort_values(by=\"metric\")\n",
    "    return pd.DataFrame(sort.is_GOP).reset_index().rename(columns={\"is_GOP\":\"prob_GOP\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate probabilites of historical data\n",
    "prob_gops = agg_probs(districts)\n",
    "#attach associated probabilities\n",
    "districts = pd.merge(districts,prob_gops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty columns to fill in the state dataset\n",
    "all_states[\"dist_loss\"] = 0\n",
    "all_states[\"porp_loss\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness(year):\n",
    "    '''A function to comupte the fairness metric for a given district\n",
    "    Args: \n",
    "        a year\n",
    "    Returns:\n",
    "        a row in a state dataframe with a ton more columns showing fairness and displaying text\n",
    "    '''\n",
    "    #pull in datasets\n",
    "    d = districts[districts[\"year\"] == year]\n",
    "    s = all_states[all_states[\"year\"] == year]\n",
    "    for ST in s.ST:\n",
    "        st = d[d[\"ST\"] == ST]\n",
    "        st_avg = st.prob_GOP.mean()\n",
    "        num_dist = len(st)\n",
    "        act_num = st_avg * num_dist\n",
    "        st_lean = s[\"metric\"][s[\"ST\"] == str(ST)]\n",
    "        best_num = st_lean * num_dist\n",
    "        diff_dist = (best_num-act_num)\n",
    "        diff_pop = (diff_dist / num_dist)\n",
    "        s.dist_loss[s[\"ST\"] == ST] = diff_dist\n",
    "        s.porp_loss[s[\"ST\"] == ST] = diff_pop\n",
    "        s[\"porp_text\"] = np.where(s[\"porp_loss\"] <= 0, '% Excluded, Against Dems', '% Excluded, Against GOP')\n",
    "        s[\"porp_text\"] = ((s[\"porp_loss\"].abs().round(2))*100).fillna(0).astype(int).astype(str).str.rstrip(\".0\") + s[\"porp_text\"]\n",
    "        s[\"porp_text\"] = np.where(s[\"porp_loss\"].isna(), 'Incomplete Map', s[\"porp_text\"])\n",
    "        s[\"dist_text\"] = np.where(s[\"dist_loss\"] <= 0, ' Dem Districts Lost', ' GOP Districts Lost')\n",
    "        s[\"dist_text\"] = s[\"dist_loss\"].abs().round(2).astype(str).str.rstrip(\".0\") + s[\"dist_text\"]\n",
    "        s[\"lean\"] = np.where(s[\"porp_loss\"] <= 0, 'Dems Excluded', 'GOP Excluded')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce Dataset for hexmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the 118 dataframe for major displays\n",
    "state_118 = fairness(2022)\n",
    "#highlight single districts and preview maps\n",
    "sds_118 = [\"VT\", \"DE\", \"WY\", \"ND\", \"SD\", \"AK\"]\n",
    "prev_states = [\"NH\",\"MO\",\"NY\",\"KS\"]\n",
    "state_118['State Status'] = 'Multi-District'\n",
    "state_118['State Status'] = np.where(state_118[\"ST\"].isin(sds_118), 'Single District', state_118['State Status'])\n",
    "state_118['State Status'] = np.where(state_118[\"ST\"].isin(prev_states), 'Incomplete Map', state_118['State Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import hexagon locations and pair them\n",
    "hexes = pd.read_excel(\"/Users/xavier/Desktop/DSPP/solo_projects/redistricting_project/raw_data/hexmap_plots.xlsx\")\n",
    "hexes = hexes.rename(columns={\"Abbreviation\":\"ST\"})\n",
    "hexes[\"row\"] = hexes[\"Row\"]*-1\n",
    "hexes[\"column\"] = hexes[\"Column\"]\n",
    "prez = state_118.merge(hexes, on=\"ST\")\n",
    "#invert hexes so map is upright\n",
    "#drop and rename columns\n",
    "prez = prez.drop(columns=[\"Row\",\"Column\",\"year\"])\n",
    "prez = prez.set_axis(['ST', 'PVI', 'Share GOP','dist_loss', 'porp_loss','Population Excluded', 'Districts Lost','Lean', 'State Status','State', 'row', 'column'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform fairness on the current congress\n",
    "state_117 = fairness(2020)\n",
    "ref_117 = state_117.merge(hexes, on=\"ST\")\n",
    "ref_117 = ref_117.drop(columns=[\"Row\",\"Column\",\"year\"])\n",
    "ref_117\n",
    "ref_117 = ref_117.set_axis(['ST', 'PVI', 'Share GOP','dist_loss_117', 'porp_loss_117','Districts Lost (117th)', 'Population Excluded (117th)','Lean (117th)', 'State', 'row', 'column'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attach 117th to it\n",
    "add_to_prez = ref_117[[\"ST\", \"dist_loss_117\", \"porp_loss_117\", \"Population Excluded (117th)\", \"Districts Lost (117th)\", 'Lean (117th)']]\n",
    "prez = prez.merge(add_to_prez)\n",
    "#Create an improvement metric\n",
    "prez[\"Change\"] = np.where(abs(prez[\"porp_loss\"]) < abs(prez[\"porp_loss_117\"]), 'Less Partisan Than 2020', 'More Partisan Than 2020')\n",
    "prez[\"Change\"] = np.where(prez[\"Population Excluded\"] == prez[\"Districts Lost (117th)\"], 'No Change Since 2020', prez[\"Change\"])\n",
    "prez[\"Change\"] = np.where(prez[\"State Status\"] == \"Incomplete Map\", 'Incomplete Map', prez[\"Change\"])\n",
    "prez[\"Change\"] = np.where(prez[\"State Status\"] == \"Single District\", 'No Change Since 2020', prez[\"Change\"])\n",
    "prez['Districts Lost'] = np.where(prez[\"State Status\"] == \"Incomplete Map\", 'Incomplete Map', prez['Districts Lost'])\n",
    "prez['Population Excluded'] = np.where(prez[\"State Status\"] == \"Incomplete Map\", 'Incomplete Map', prez['Population Excluded'])\n",
    "prez['Districts Lost'] = np.where(prez[\"State Status\"] == \"Incomplete Map\", 'Incomplete Map', prez['Districts Lost'])\n",
    "prez['porp_loss'] = np.where(prez[\"State Status\"] == \"Incomplete Map\", np.nan, prez['porp_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prez[prez[\"Population Excluded\"] == prez[\"Districts Lost (117th)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix Error States\n",
    "prez[\"Population Excluded (117th)\"][prez[\"ST\"] == \"NV\"] = \"Perfect Match\"\n",
    "#prez[\"Population Excluded\"][prez[\"State\"] == \"Ohio (Overturned)\"] = \"20% Excluded, Against Dems\"\n",
    "#prez[\"Population Excluded\"][prez[\"ST\"] == \"AZ\"] = \"10% Excluded, Against Dems\"\n",
    "#prez[\"Population Excluded\"][prez[\"ST\"] == \"DE\"] = \"40% Excluded, Against GOP\"\n",
    "prez[\"Population Excluded (117th)\"][prez[\"ST\"] == \"DE\"] = \"40% Excluded, Against GOP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18,)\n",
      "(18,)\n"
     ]
    }
   ],
   "source": [
    "#import data from the previous dfs\n",
    "#import strings from the old sheet, with headers and the two states\n",
    "Cols = \"ST,PVI,Share GOP,dist_loss,porp_loss,State,row,column,Population Excluded,pe_extra,Districts Lost,State Status,porp_loss_bin,Lean,dist_loss_117,porp_loss_117,Population Excluded (117th),pe_extra_117,Districts Lost (117th),State Status (117th),porp_loss_bin_117,Lean (117th),Change\"\n",
    "Cols = Cols.split(\",\")\n",
    "XOH = \"OH,R+6,0.56,-2.927585787909539,-0.19517238586063593,Ohio (Overturned),0,5.5,20% Excluded, Against Dems,2.93 Dem Districts Lost,Overturned Map,-2.0,Dems Excluded,-2.3115955431306805,-0.14447472144566753,14% Excluded, Against Dems,2.31 Dem Districts Lost,Multi-District,-1,Dems Excluded,More Partisan Than 2020\"\n",
    "XOH = pd.DataFrame(XOH.split(\",\")).T\n",
    "XNC = \"NC,R+3,0.53,-2.2426418948429436,-0.16018870677449598,North Carolina (Overturned),0,6.5,16% Excluded, Against Dems,2.24 Dem Districts Lost,Overturned Map,-2.0,Dems Excluded,-0.7676470696329112,-0.05904977458714701,6% Excluded, Against Dems,0.77 Dem Districts Lost,Multi-District,-1,Dems Excluded,More Partisan Than 2020\"\n",
    "XNC = pd.DataFrame(XNC.split(\",\")).T\n",
    "XMD = \"MD,D+14,0.36,2.028625634214295,0.2535782042767869,Maryland (Overturned),0,7.5,25% Excluded, Against GOP,2.03 GOP Districts Lost,Overturned Map,3,GOP Excluded,1.9042442870245693,0.23803053587807116,24% Excluded, Against GOP,1.9 GOP Districts Lost,Multi-District,2,GOP Excluded,More Partisan Than 2020\"\n",
    "XMD = pd.DataFrame(XMD.split(\",\")).T\n",
    "#potentially add in NY depending on court results\n",
    "#merge the two frames and fix column names\n",
    "ovs = pd.concat([XOH,XNC,XMD])\n",
    "ovs.columns=Cols\n",
    "#merge comma deliniated cells back together\n",
    "ovs[\"Population Excluded\"] = ovs[\"Population Excluded\"] + \",\" + ovs[\"pe_extra\"]\n",
    "ovs[\"Population Excluded (117th)\"] = ovs[\"Population Excluded (117th)\"] + \", \" + ovs[\"pe_extra_117\"]\n",
    "#drop columns no longer in the prez\n",
    "ovs = ovs.drop(columns=[\"pe_extra\",\"pe_extra_117\",\"porp_loss_bin\", \"State Status (117th)\",\"porp_loss_bin_117\"])\n",
    "print(ovs.columns.shape)\n",
    "print(prez.columns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#first, fix the basic zero rounding erros\n",
    "prez[\"Population Excluded\"] = np.where(prez[\"Population Excluded\"].str[0]==\"%\", (\"0\" + prez[\"Population Excluded\"]), prez[\"Population Excluded\"])\n",
    "\n",
    "#identify possible string rounding errors in the master\n",
    "\n",
    "# limit rows to those with a single digit reported percent\n",
    "low_pct = prez[prez[\"Population Excluded\"].str[1] == \"%\"]\n",
    "\n",
    "# further limit to only those with possible rounding errors\n",
    "low_pct = low_pct[low_pct[\"Population Excluded\"].str[0].astype(int) < 5]\n",
    "\n",
    "#now eliminate all correct rows that show a low percent \n",
    "low_pct = low_pct[round(abs(low_pct[\"porp_loss\"]),2).astype(str).str[2] != \"0\"]\n",
    "\n",
    "#All of the remaining rows have the rounding error that excludes the zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_round_ST(ST):\n",
    "    '''A function that adds a zero to a string when the rounding excluded a 0\n",
    "    Args:\n",
    "        A state\n",
    "    Returns:\n",
    "        The associated row in the master dataframe now gets its deserved zero\n",
    "    \n",
    "    '''\n",
    "    prez[\"Population Excluded\"][prez[\"ST\"] == ST] = (prez[\"Population Excluded\"].str[0] + \"0\" + prez[\"Population Excluded\"].str[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply the fundtion to all erroneous year-state combinations\n",
    "[fix_round_ST(ST) for ST in low_pct.ST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge back into the set\n",
    "prez = pd.concat([prez,ovs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "prez.to_csv(\"/Users/xavier/Desktop/DSPP/solo_projects/redistricting_project/processed/2022_redraw_workbook.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prez = prez[prez[\"State Status\"] != \"Overturned Map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a quick state name key\n",
    "abbrev = prez[[\"ST\",\"State\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce dataset for Slider Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = [fairness(year) for year in all_states.year.unique()]\n",
    "master = pd.concat(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = master.merge(abbrev, on=\"ST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column that verbally represents the congress\n",
    "master[\"congress\"] = (.5*(master[\"year\"])-893).astype(str).str.rstrip(\".0\")\n",
    "master[\"Period\"] = master[\"congress\"] + \"th Congress (\" + master[\"year\"].astype(str) + \")\"\n",
    "master = master.replace('11th Congress (2006)','110th Congress (2006)')\n",
    "master[\"congress\"] = master[\"congress\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catalogue Redistricting Events\n",
    "master[\"event\"] = \"No Change Since Last Election\"\n",
    "master[\"event\"] = np.where(master[\"year\"] == 2004, '2004 Election and TX redraw', master[\"event\"])\n",
    "master[\"event\"] = np.where(master[\"year\"] == 2008, '2008 Election', master[\"event\"])\n",
    "master[\"event\"] = np.where(master[\"year\"] == 2012, '2012 Election and Redistricting', master[\"event\"])\n",
    "master[\"event\"] = np.where(master[\"year\"] == 2016, '2016 Election and FL/NC/VA redraw', master[\"event\"])\n",
    "master[\"event\"] = np.where(master[\"year\"] == 2018, 'PA Redraw', master[\"event\"])\n",
    "master[\"event\"] = np.where(master[\"year\"] == 2020, '2020 Election', master[\"event\"])\n",
    "master[\"event\"] = np.where(master[\"year\"] == 2022, '2022 Redistricting', master[\"event\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place identifying strings on maps not yet passed\n",
    "master[\"State\"] = np.where((master[\"ST\"].isin(prev_states)) & (master[\"year\"]==2022), (master[\"State\"] + \" (Anticipated)\"), master[\"State\"])\n",
    "#master[\"ST\"] = np.where((master[\"ST\"].isin(prev_states)) & (master[\"year\"]==2022), (master[\"ST\"] + \" (Anticipated)\"), master[\"ST\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute aggregate loss of districts and add it to the dataframe\n",
    "# create dataset for total_dist_loss_lean with an abbreviated name\n",
    "tdll = master.groupby([\"year\",\"lean\"]).sum().reset_index().drop([\"metric\",\"congress\",\"porp_loss\"],axis=1)\n",
    "tdll[\"ST\"] = \"US\"\n",
    "\n",
    "# create dataset for total_dist_loss_year with an abbreviated name\n",
    "tdly = master.groupby([\"year\"]).sum().reset_index().drop([\"metric\",\"congress\",\"porp_loss\"],axis=1)\n",
    "\n",
    "#incorporate into main \n",
    "tdll = tdll.rename(columns={\"dist_loss\":\"dist_loss_agg\"})\n",
    "tdly = tdly.rename(columns={\"dist_loss\":\"dist_loss_net\"})\n",
    "year_lean_measures = pd.merge(tdll,tdly,on=\"year\")\n",
    "agg_dl = year_lean_measures\n",
    "agg_dl[\"key\"] = agg_dl[\"year\"].astype(str) + agg_dl[\"lean\"].str[0]\n",
    "agg_dl = agg_dl.drop([\"year\",\"lean\",\"ST\"], axis = 1)\n",
    "master[\"key\"] = master[\"year\"].astype(str) + master[\"lean\"].str[0]\n",
    "master = pd.merge(master,agg_dl)\n",
    "master = master.drop([\"key\"], axis = 1)\n",
    "\n",
    "tdll.to_csv(\"/Users/xavier/Desktop/DSPP/solo_projects/redistricting_project/processed/total_dist_loss_lean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#first, fix the basic zero rounding erros\n",
    "master[\"porp_text\"] = np.where(master[\"porp_text\"].str[0]==\"%\", (\"0\" + master[\"porp_text\"]), master[\"porp_text\"])\n",
    "\n",
    "#identify possible string rounding errors in the master\n",
    "\n",
    "# limit rows to those with a single digit reported percent\n",
    "low_pct = master[master[\"porp_text\"].str[1] == \"%\"]\n",
    "\n",
    "# further limit to only those with possible rounding errors\n",
    "low_pct = low_pct[low_pct[\"porp_text\"].str[0].astype(int) < 5]\n",
    "\n",
    "#now eliminate all correct rows that show a low percent \n",
    "low_pct = low_pct[round(abs(low_pct[\"porp_loss\"]),2).astype(str).str[2] != \"0\"]\n",
    "\n",
    "#All of the remaining rows have the rounding error that excludes the zero\n",
    "#export a list of the year and state\n",
    "low_pct[\"STyear\"] = (low_pct[\"ST\"]+low_pct[\"year\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_round(STyear):\n",
    "    '''A function that adds a zero to a string when the rounding excluded a 0\n",
    "    Args:\n",
    "        A year and a state\n",
    "    Returns:\n",
    "        The associated row in the master dataframe now gets its deserved zero\n",
    "    \n",
    "    '''\n",
    "    ST = STyear[0:2]\n",
    "    year = int(STyear[2:])\n",
    "    master[\"porp_text\"][(master[\"ST\"] == ST) & (master[\"year\"] == year)] = (master[\"porp_text\"].str[0] + \"0\" + master[\"porp_text\"].str[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>ST</th>\n",
       "      <th>PVI</th>\n",
       "      <th>metric</th>\n",
       "      <th>dist_loss</th>\n",
       "      <th>porp_loss</th>\n",
       "      <th>porp_text</th>\n",
       "      <th>dist_text</th>\n",
       "      <th>lean</th>\n",
       "      <th>State</th>\n",
       "      <th>congress</th>\n",
       "      <th>Period</th>\n",
       "      <th>event</th>\n",
       "      <th>dist_loss_agg</th>\n",
       "      <th>dist_loss_net</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004</td>\n",
       "      <td>AL</td>\n",
       "      <td>R+8</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-1.403359</td>\n",
       "      <td>-0.20048</td>\n",
       "      <td>20% Excluded, Against Dems</td>\n",
       "      <td>1.4 Dem Districts Lost</td>\n",
       "      <td>Dems Excluded</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>109</td>\n",
       "      <td>109th Congress (2004)</td>\n",
       "      <td>2004 Election and TX redraw</td>\n",
       "      <td>-27.822022</td>\n",
       "      <td>-2.335246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  ST  PVI  metric  dist_loss  porp_loss                   porp_text  \\\n",
       "0  2004  AL  R+8    0.58  -1.403359   -0.20048  20% Excluded, Against Dems   \n",
       "\n",
       "                dist_text           lean    State  congress  \\\n",
       "0  1.4 Dem Districts Lost  Dems Excluded  Alabama       109   \n",
       "\n",
       "                  Period                        event  dist_loss_agg  \\\n",
       "0  109th Congress (2004)  2004 Election and TX redraw     -27.822022   \n",
       "\n",
       "   dist_loss_net  \n",
       "0      -2.335246  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply the fundtion to all erroneous year-state combinations\n",
    "[fix_round(STyear) for STyear in low_pct.STyear]\n",
    "#check AL in 2004\n",
    "master.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.to_csv(\"/Users/xavier/Desktop/DSPP/solo_projects/redistricting_project/processed/aggregated_fairness.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spare Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
